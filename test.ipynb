{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import scipy.sparse as sp\n",
    "import resource\n",
    "import time\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import json, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import ReviewCoverLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "    session = tf.Session(config=config)\n",
    "    session\n",
    "set_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputSize_wh = 32\n",
    "batch_size = 32\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = pd.DataFrame()\n",
    "accuracy_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_names = ['test_number', 'pad_Test', 'test_num2resizedImg', 'test_obj', 'test_label']\n",
    "test_number,pad_Test,test_num2resizedImg,test_obj,test_label = [], [], [], [], []\n",
    "test_files = [test_number,pad_Test,test_num2resizedImg,test_obj,test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"test_data/preprocessed_data/test_number.h5\",\"rb\") as f:test_number = pickle.load(f)\n",
    "with open(f\"test_data/preprocessed_data/pad_Test.h5\",\"rb\") as f:pad_Test = pickle.load(f)\n",
    "with open(f\"test_data/preprocessed_data/test_num2resizedImg.h5\",\"rb\") as f:test_num2resizedImg = pickle.load(f)\n",
    "with open(f\"test_data/preprocessed_data/test_obj.h5\",\"rb\") as f:test_obj = pickle.load(f)\n",
    "with open(f\"test_data/preprocessed_data/test_label.h5\",\"rb\") as f:test_label = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, name in enumerate(test_file_names):\n",
    "    with open(f\"test_data/preprocessed_data/{name}.h5\",\"rb\") as f:\n",
    "        test_files[ind] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=tf.keras.models.load_model(\"main_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_predict = model1.predict_generator(ReviewCoverLoader(\n",
    "    test_number,pad_Test,test_num2resizedImg,test_obj,test_label, InputSize_wh, batch_size,shuffle=False))\n",
    "report_score = classification_report(test_label[:len(fused_predict)], fused_predict.argmax(axis=1),digits=4, output_dict=True)\n",
    "accuracy_avg+=report_score[\"accuracy\"]\n",
    "report_score = pd.DataFrame(report_score)\n",
    "\n",
    "if(ind==0):total_result = report_score\n",
    "else:total_result += report_score\n",
    "\n",
    "print('\\n',report_score.T)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_mental_py36",
   "language": "python",
   "name": "gpu_mental_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
